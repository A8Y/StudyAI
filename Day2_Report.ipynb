{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2_Report.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMMpKMEMJ0RjQFuKAtcrisn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A8Y/StudyAI/blob/master/Day2_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv0JCXUxB0ks",
        "colab_type": "text"
      },
      "source": [
        "# DNN　Day2レポート\n",
        "\n",
        "Day２では前回に続き勾配消失問題や学習率最適化法などをおさえてから、CNNにはいる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCVJDcTsCD5u",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 動画まとめと考察：\n",
        "\n",
        "##  Section1: 勾配消失問題\n",
        ">　誤差逆伝播が下位層に行くにしたがって、勾配がどんどん緩やかになっていく。よって、勾配降下法では下位層パラメータはほとんど変わらなくなり、モデル学習は最適化されない。\n",
        "\n",
        "### **勾配消失解決法**\n",
        "\n",
        "#####   1. **活性化関数を変える**（Sigmoid -> ReLU）\n",
        "\n",
        "#####   2. **重みの初期値設定**: 　\n",
        "完全にランダムな方法で重みを初期化するのではなく、その構造の特徴を考慮に入れて初期化する方法\n",
        "\n",
        "1.  Xavier ---- 重みの要素を前層のノード数の平方根で除算した値\n",
        "\n",
        "\n",
        "*   Xavierを使うときの活性化関数 (Relu, Sigmoid, tanh)\n",
        "\n",
        "2.  He ---- 重みの要素を前層のノード数の平方根で除算した値に対しsqrt(2)を掛け合わせたもの\n",
        "\n",
        "*   Heを使うときの活性化関数 ( Relu  )\n",
        "\n",
        "#####   3. **バッチ正規化**\n",
        "\n",
        "*   より高い学習率を利用可能にし初期化への強い依存を減らす\n",
        "\n",
        "*   ミニバッチ単位で入力値のデータの偏りを抑制する手法\n",
        "\n",
        "\n",
        "*   非線形化の**実行前**にバッチ正規化を孕んだ層を加える\n",
        "\n",
        "##  Section2　　適応学習率法\n",
        "\n",
        "#### モメンタム\n",
        "\n",
        "誤差をパラメータで微分したものと学習率の席を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する\n",
        "\n",
        "* 　グローバル最適解となる\n",
        "\n",
        "*   慣性：　0.05～0.09　一般的\n",
        "\n",
        "#### AdaGrad\n",
        "\n",
        "*   誤差をパラメータで微分したものと再定義した学習率の積を減算する\n",
        "\n",
        "*Pros*: \n",
        "\n",
        "*   勾配の緩やかな斜面に対して、最適値に近づける\n",
        "\n",
        "*Cons*: \n",
        "\n",
        "*   学習率が徐々に小さくなるので鞍点問題を引き起こす\n",
        "\n",
        "#### RMSProp\n",
        "\n",
        "*   誤差をパラメータで微分したものと再定義した学習率の積を減算する\n",
        "\n",
        "*   ローカル最適解にはならず、グローバル最適解になる場合が多い\n",
        "\n",
        "*   ハイパーパラメータの調整が必要な場合が少ない\n",
        "\n",
        "*   AdaGradのデメリットを改善\n",
        "\n",
        "#### Adam\n",
        "\n",
        "*   モメンタムの過去の勾配の指数関数的減数平均\n",
        "\n",
        "*   RMSPropの過去の勾配の二乗の指数関数的減衰平均\n",
        "\n",
        "*   モメンタムとRMSPropのメリットを孕んだ手法\n",
        "\n",
        "\n",
        "##  Section3  過学習\n",
        "\n",
        "*原因*：\n",
        "\n",
        "*  重みが大きい値をとることで過学習が発生\n",
        "  1. パラメータ数が多い\n",
        "\n",
        "  2. パラメータの値が適切でない\n",
        "\n",
        "  3. ノード数が多い\n",
        "\n",
        "  4. 層が多い\n",
        "\n",
        "---> ネットワークの自由度(層数、ノード数、パラメータ値）が高い\n",
        "\n",
        "*解決策*：\n",
        "\n",
        "過学習が起こりそうな重みの大きさ以下で重みをコントロールし、重みのばらつきを出す\n",
        "\n",
        "\n",
        "**正則化手法で過学習をコントロール**\n",
        "\n",
        "正則化とはネットワークの自由度を制約すること\n",
        "\n",
        "1.  Lasso:　*L1正則化*\n",
        "\n",
        "*     スパース推定\n",
        "\n",
        "2.  Ridge: 　*L2正則化*\n",
        "\n",
        "*   縮小推定\n",
        "\n",
        "**ドロップアウト**\n",
        "\n",
        "ノード数が多すぎる場合の解決法\n",
        "\n",
        "ランダムにノードを削除して学習させる\n",
        "\n",
        "データ量を変化させずに異なるモデルを学習させていると解釈できる\n",
        "\n",
        "Weight Decay (荷重減衰)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMUPU4DCRVdj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##  Section4　CNN\n",
        "\n",
        "層の種類：\n",
        "\n",
        "1.   畳込み層\n",
        "\n",
        "2.   プーリング層\n",
        "\n",
        "3.   全結合\n",
        "\n",
        "### **畳み込み層**\n",
        "\n",
        "畳み込み層では、画像の場合、縦、横、チャンネルの3次元のデータをそのまま学習し、次に伝えることができる\n",
        "\n",
        "3次元の空間情報も学習できるのがCNN\n",
        "\n",
        "*   フィルター: 特徴マップ（活性化マップ）を生成\n",
        "\n",
        "*   バイアス：\n",
        "\n",
        "*   パディング：　入力の各境界に対してｐ個のゼロを追加するプロセス\n",
        "\n",
        "*   ストライド：　各操作の後にウィンドウを移動させるピクセル数\n",
        "\n",
        "*   チャンネル：　\n",
        "\n",
        "### **プーリング層**\n",
        "\n",
        "プーリング層は位置不変性をもつ縮小操作で、通常は畳み込み層の後に適用される。\n",
        "\n",
        "**Max Pooling**\n",
        "\n",
        "各プーリング操作は現在のビューの中から最大値を選ぶ\n",
        "\n",
        "• 検出された特徴を保持する\n",
        "• 最も一般的に利用される\n",
        "\n",
        "**Average Pooling**\n",
        "\n",
        "各プーリング操作は現在のビューに含まれる値を平均する\n",
        "\n",
        "• 特徴マップをダウンサンプリングする\n",
        "• LeNetで利用される\n",
        "\n",
        "##  Section5　最新CNN\n",
        "\n",
        "**AlexNet**\n",
        "\n",
        "*  2012年に発表されたDNNが注目されるきっかけを作ったモデル\n",
        "\n",
        "*  画像認識精度を格段に上げた\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsJk6Z_vCpeb",
        "colab_type": "text"
      },
      "source": [
        "## 確認テスト\n",
        "\n",
        "【p12】　$ z= t^2 \\quad t = x+y$ の$dz/dx$をもとめよ\n",
        "$$\\frac{dz}{dt} = 2t$$\n",
        "$$\\frac{dt}{dx} = 1$$\n",
        "$$\\frac{dz}{dx}=\\frac{dz}{dt}\\frac{dt}{dx}=(2t)(1)=2(x+y)$$\n",
        "\n",
        "【p20】\n",
        "\n",
        "2\n",
        "\n",
        "【p】\n",
        "\n",
        "入力値が反映されずすべての出力が同じになる\n",
        "\n",
        "【p31】一般的なバッチ正規化の効果2点\n",
        "\n",
        "1.   正規化によってデータのばらつきが少なくなり、計算の高速化が考えられる\n",
        "2.   勾配消失が起きづらくなる\n",
        "\n",
        "\n",
        "【p35】例題チャレンジ\n",
        "\n",
        "\n",
        "【p47】\n",
        "\n",
        "**モメンタム**\n",
        "\n",
        "*   ローカル最適解を避け、グローバル最適解となる\n",
        "\n",
        "*   谷間についてから収束するのが速い\n",
        "\n",
        "\n",
        "**AdaGrad**\n",
        "\n",
        "*   緩やかな斜面においても、最適値に近づけやすい\n",
        "\n",
        "**RMSPropの特徴**\n",
        "\n",
        "*   パラメータの調整が少なくてよい\n",
        "\n",
        "【p63】\n",
        "\n",
        "(a)\n",
        "\n",
        "【p68】\n",
        "\n",
        "Lasso\n",
        "\n",
        "\n",
        "【p100】\n",
        "\n",
        "7x7\n",
        "\n",
        "Output_height = $\\frac{input height + 2*pooling - filter height}{stride} +1$\n",
        "\n",
        "Output_width = $\\frac{input width + 2*pooling - filter width}{stride} +1$\n",
        "\n"
      ]
    }
  ]
}