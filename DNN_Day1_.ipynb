{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN_Day1_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNz6OaTMa/eS2m45pF36QSE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A8Y/StudyAI/blob/master/DNN_Day1_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbZdhxoYtV3z",
        "colab_type": "text"
      },
      "source": [
        "# DNN Day1\n",
        "\n",
        "Day1では基礎的な全結合DNNを扱う\n",
        "\n",
        "\n",
        "全結合DNNの全体像\n",
        "\n",
        "![alt text](https://miro.medium.com/max/2460/1*KHs1Chs6TCJDTIIQVyIJxg.png)\n",
        "参照：[towardsdatascience.com](https://towardsdatascience.com/a-laymans-guide-to-deep-neural-networks-ddcea24847fb)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb6UhQCotaNg",
        "colab_type": "text"
      },
      "source": [
        "## Section 1: 入力層～中間層　(Input ~ Hidden Layers)\n",
        "\n",
        "n個の特徴値をもつ$x$を入力。中間層１のノード１に入力されて次の層へ出力されるダイアグラム\n",
        "\n",
        "![alt text](https://matlabgeeks.com/wp-content/uploads/2011/05/Perceptron.bmp)\n",
        "\n",
        "参照：https://matlabgeeks.com\n",
        "\n",
        ">与えられた情報\n",
        "\n",
        "* 入力値　$x = \\{ x_i : i = 1, ..., m \\}$\n",
        "\n",
        "> パラメータ\n",
        "\n",
        "* 重み $W=[w_{ji}], i=1...,I$ and $j=1,...J$\n",
        "\n",
        "* バイアス　$b$\n",
        "\n",
        "> 中間層への入力 $u_j= \\Sigma_i (w_{ji} x_i + b_j)$\n",
        "\n",
        "> 中間層からの出力　$z_j = f(u_j)$\n",
        "\n",
        "中間層の数とノードの数は任意で決めてよいが、多いからといってよいモデルとは限らない。\n",
        "\n",
        "ノード数を増やすことによって精度をあげることが期待できるが、増やしすぎると計算機のメモリーを使いすぎる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck75E08-taV8",
        "colab_type": "text"
      },
      "source": [
        "## Section 2 活性化関数導入 \n",
        "\n",
        "前の層から受け取った値を非線形に変換させる関数　$f(.)$で、次の層への出力値の大きさを決める\n",
        "\n",
        "$$z = f(u)$$\n",
        "\n",
        "\n",
        "入力値の値によって、次の層への信号のON/OFFや強弱を定める働きを持つ\n",
        "\n",
        "### 活性化関数\n",
        "\n",
        "####中間層用の活性化関数\n",
        "\n",
        ">**ReLU関数** : \n",
        "\n",
        "$$f(u) = \n",
        "  \\begin{cases}\n",
        "    u \\quad if \\quad u>0 \\\\\n",
        "    0 \\quad else\n",
        "  \\end{cases}\n",
        "$$\n",
        "\n",
        "* 今最も使われている\n",
        "\n",
        "* 勾配消失問題回避とスパース化に貢献することでよい結果をもたらしている\n",
        "\n",
        "* しかし、これに固執せずに状況によって使い分けるのがよい\n",
        "\n",
        "\n",
        "> **シグモイド（ロジスティック）関数**\n",
        "\n",
        "$$f(u) = \\frac{1}{1+e^{-u}}$$\n",
        "\n",
        "* ０～１間を緩やかに変化する関数で、状態に対し、信号の強弱を伝えられるようになり、予想NN普及のきっかけとなった\n",
        "\n",
        "* **勾配消失問題**：　大きな値では出力な変化が微細なため、勾配消失が起こることがある\n",
        "\n",
        "* 小さな数字が０にならないため、計算上メモリーを消費する\n",
        "\n",
        "> **ステップ関数**（今はDLでは使われていない）\n",
        "\n",
        "*閾値を越えたら発火する関数であり、出力は常に１か０．\n",
        "\n",
        "*パーセプトロンで利用された関数\n",
        "\n",
        "*０－１の間を表現できず、線形問題しか解決できなかった\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkaxBja4tabr",
        "colab_type": "text"
      },
      "source": [
        "## Section 3　出力層\n",
        "\n",
        "###出力層の役割\n",
        "\n",
        "中間層までの数字を人間が見て意味ある数値に変換\n",
        "\n",
        "うまく学習がなされていない場合は数字にばらつきがない\n",
        "\n",
        "あるカテゴリーに確率１で分類されるときは過学習が起きている可能性がある\n",
        "\n",
        "####出力層用の活性化関数\n",
        "\n",
        "\n",
        "> **恒等写像**　：　回帰問題\n",
        "\n",
        "> **シグモイド関数**　：　2項分類問題\n",
        "\n",
        "> **ソフトマックス関数**　多項分類問題\n",
        "\n",
        "$$f(i, u) = \\frac{ e^{u_i} }{\\Sigma_{k=1} ^K e^{u_k}} $$\n",
        "\n",
        "#####値の強弱\n",
        "\n",
        "* 中間層：　しきい値の前後で信号の強弱を調整\n",
        "\n",
        "* 出力層：　信号の大きさ(比率）はそのままに変換\n",
        "\n",
        "##### 確率出力\n",
        "\n",
        "* 分類問題の場合、出力層の出力は０～１で総和を1とする\n",
        "\n",
        "#### 誤差関数：　モデルの出力が実際の正解値をどの程度正確に予測できているかを評価する\n",
        "\n",
        "\n",
        "* 回帰問題:  　2乗誤差\n",
        "\n",
        "* 2項分類:  　 交差エントロピー\n",
        "\n",
        "* 多項分類:   　交差エントロピー\n",
        "\n",
        "\n",
        "訓練データサンプルあたりの誤差\n",
        "\n",
        "> 2乗誤差: $E_n(W) = \\frac{1}{2} \\Sigma_{i=1} ^I (y_n - d_n) ^2$\n",
        "\n",
        "> 交差エントロピー: $E_n(W) = - \\Sigma_{i=1} ^I d_i \\log y_i$\n",
        "\n",
        "学習サイクルあたりの誤差\n",
        "\n",
        "$$E(W) = \\Sigma _{n=1} ^N E_n$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GVTPkGDtae1",
        "colab_type": "text"
      },
      "source": [
        "## Section 4　勾配降下法\n",
        "\n",
        "深層学習の目的\n",
        "\n",
        "* 学習を通して誤差を最小にするネットワークを作成すること\n",
        "\n",
        "* 誤差を最小化する最適な重みを探索すること\n",
        "\n",
        "#### **勾配降下法**　（全サンプルの平均誤差を最小化するパラメータ推定法）\n",
        "\n",
        "$$W^{(t+1)} \\leftarrow  W^{(t)} - \\epsilon \\Delta E$$\n",
        "\n",
        "$$\\Delta E = \\frac{\\partial E}{\\partial w}=[ \\frac{\\partial E}{\\partial w_1},..., \\frac{\\partial E}{\\partial w_M} ] $$\n",
        "\n",
        "$\\epsilon$ を**学習率**という。　学習率とは重みの更新速度を表している。\n",
        "\n",
        "* 学習率が大きすぎると発散して最小値に到達しない\n",
        "\n",
        "* 学習率が小さすぎると最小値に到着するまでに時間がかかりすぎたり、ローカルオプティマムにおちいって、グローバル最小値にとうちゃくしないことがある\n",
        "\n",
        "*  誤差関数の値をより小さくする方向に重み及びバイアスを更新し次回のエポックに反映\n",
        "\n",
        "* 学習率は定数とすることもできるが、適応的に変更することもできる。\n",
        "\n",
        "#### **適応学習率手法** ：\n",
        "モデル学習中に学習率を変動させることにより、学習時間短縮や精度向上につなげる\n",
        "\n",
        "\n",
        "* Momentum\n",
        "\n",
        "* AdaGrad\n",
        "\n",
        "* Adadelta\n",
        "\n",
        "* Adam　:　最も人気の手法\n",
        "\n",
        "色々あるが、これはDay2で\n",
        "\n",
        "\n",
        "#### **確率的勾配降下法**（SGD)\n",
        "\n",
        "$$W^{(t+1)} = W^{(t)} - \\epsilon \\Delta E_n$$\n",
        "\n",
        "\n",
        "* ランダムに抽出したサンプルの誤差\n",
        "\n",
        "* 冗長なデータの計算コストの軽減\n",
        "\n",
        "* 望まない局所極小解に収束するリスクの軽減\n",
        "\n",
        "* オンライン学習ができる\n",
        "\n",
        "### **ミニバッチ勾配降下法**\n",
        "\n",
        "ランダムに分割したデータの集合（みにバッチ）$D_t$に属するサンプルの平均誤差\n",
        "\n",
        "$$W^{(t+1)} = W^{(t)} - \\epsilon \\Delta E_t$$\n",
        "\n",
        "$$E_t = \\frac{1}{N_t} \\Sigma _{n \\in D_t} E_n$$\n",
        "\n",
        "$$N_t = |D_t|$$\n",
        "\n",
        "確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる\n",
        "\n",
        "\n",
        "誤差勾配の計算　：　\n",
        "\n",
        "数値微分：計算がエポックごとにおおきく、扱いづらい.　よって、**誤差逆伝播法**で推定\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOoRlxrtBmxE",
        "colab_type": "text"
      },
      "source": [
        "#Section 5: 誤差逆伝播法\n",
        "\n",
        "実際の出力値とその期待値の差に基づいて重みを更新する手法\n",
        "\n",
        "算出された誤差を出力層から順に微分し前の層へと伝播。各重みに関する微分は連鎖率を持ちて計算される。\n",
        "\n",
        "最小限の計算で各パラメータの微分値を解析的に計算する方法。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzS02QeM3ebp",
        "colab_type": "text"
      },
      "source": [
        "# 確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8raTibOucNd",
        "colab_type": "text"
      },
      "source": [
        "###Q1．\n",
        "\n",
        "出力値の誤差を最小化するパラメータを最適化すること\n",
        "\n",
        "> 3．重み\n",
        "\n",
        "> 4．バイアス\n",
        "\n",
        "###Q2．\n",
        "\n",
        "入力層：2ノード\n",
        "\n",
        "中間層　2層:　3ノード\n",
        "\n",
        "出力層　:　1ノード\n",
        "\n",
        "###Q3. 動物の例\n",
        "\n",
        "\n",
        "###Q4.\n",
        "\n",
        "u1 = np.dot(x, W1)\n",
        "\n",
        "###Q5.\n",
        "\n",
        "u = np.dot(x, W) + b\n",
        "\n",
        "###Q6.\n",
        "\n",
        "線形:　傾きが一定、平面、境界線が直線\n",
        "\n",
        "非線形：傾きが変わり、曲線や曲面\n",
        "\n",
        "分類問題で特徴値が2個で2クラスで言うと、一本の直線で分けることができる\n",
        "\n",
        "\n",
        "###Q7.\n",
        "\n",
        "\n",
        "#### 中間層出力\n",
        "z = functions.sigmoid(u)\n",
        "\n",
        "\n",
        "\n",
        "###Q8.\n",
        "\n",
        "* 2乗することによってすべての誤差が正になりその大きさが正確に測れる\n",
        "\n",
        "*誤差関数を微分した時に降りてきた２が打ち消されるように２で割っている　（＝０で最適値を推定するので、２で割っても同じ結果はおなじであるから、計算の便宜上の為））\n",
        "\n",
        "###Q9.\n",
        "\n",
        "# ソフトマックス関数\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:  受け取った値が2次元\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)　　受け取った最大値を減算\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)　softmax関数\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # オーバーフロー対策 expの計算で機械がハンドルできる範囲に入力値を調整\n",
        "\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "    (1) = y\n",
        "\n",
        "    (2) = np.exp(x)\n",
        "\n",
        "    (3) = np.sum(np.exp(x), axis=0) \n",
        "     or np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "\n",
        "###Q10.\n",
        "\n",
        "\n",
        "# クロスエントロピー\n",
        "def cross_entropy_error(d, y):\n",
        "    if y.ndim == 1:\n",
        "        d = d.reshape(1, d.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # **教師データがone-hot-vectorの場合**、正解ラベルのインデックスに変換\n",
        "    if d.size == y.size:\n",
        "        d = d.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size\n",
        "\n",
        "（１）cross_entropy_error(d, y)\n",
        "\n",
        "（２）-np.sum(np.log(y[np.arange(batch_size), d] + 1e-7))\n",
        "\n",
        "1e-7 を足して分母が０にならないように\n",
        "\n",
        "###Q11.\n",
        "\n",
        ">  network[key]  -= learning_rate * grad[key]\n",
        "\n",
        ">  grad = backward(x, d, z1, y)\n",
        "\n",
        "###Q12.\n",
        "\n",
        "オンライン学習とは、オンラインで新しく入ってきたデータを使って既存のデータで学習したモデルをさらに学習させる\n",
        "\n",
        "###Q13.\n",
        "\n",
        "$W^{(t+1)} = W^{(t)} - \\epsilon \\Delta E_t$\n",
        "\n",
        "\n",
        "###Q14.\n",
        "\n",
        "\n",
        "def backward(x, d, z1, y):\n",
        "    print(\"\\n##### 誤差逆伝播開始 #####\")\n",
        "\n",
        "    grad = {}\n",
        "\n",
        "    W1, W2 = network['W1'], network['W2']\n",
        "    b1, b2 = network['b1'], network['b2']\n",
        "    #  出力層でのデルタ\n",
        "    delta2 = functions.d_sigmoid_with_loss(d, y)\n",
        "    #  b2の勾配\n",
        "    grad['b2'] = np.sum(delta2, axis=0)\n",
        "    #  W2の勾配\n",
        "    grad['W2'] = np.dot(z1.T, delta2)\n",
        "    #  中間層でのデルタ\n",
        "    delta1 = np.dot(delta2, W2.T) * functions.d_relu(z1)\n",
        "    # b1の勾配\n",
        "    grad['b1'] = np.sum(delta1, axis=0)\n",
        "    #  W1の勾配\n",
        "    grad['W1'] = np.dot(x.T, delta1)\n",
        "        \n",
        "    print_vec(\"偏微分_dE/du2\", delta2)\n",
        "    print_vec(\"偏微分_dE/du2\", delta1)\n",
        "\n",
        "    print_vec(\"偏微分_重み1\", grad[\"W1\"])\n",
        "    print_vec(\"偏微分_重み2\", grad[\"W2\"])\n",
        "    print_vec(\"偏微分_バイアス1\", grad[\"b1\"])\n",
        "    print_vec(\"偏微分_バイアス2\", grad[\"b2\"])\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###Q15.\n",
        "\n",
        "\n",
        "    出力層でのデルタ\n",
        "    delta2 = functions.d_mean_squared_error(d, y)\n",
        "\n",
        "    W2の勾配\n",
        "    grad['W2'] = np.dot(z1.T, delta2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnpZCc_gDrl8",
        "colab_type": "text"
      },
      "source": [
        "# 終了課題について確認　\n",
        "\n",
        "Q1. Irisデータを使って実際にNNを構築。従来の分類手法では分類できない点がある非線形問題をかかえているので、NNで精度を高めることができるかさぐってみる。\n",
        "\n",
        "Q2. 与えられた目的変数がIrisの3つの種類というカテゴリーなので、多項分類問題として扱った。与えられた特徴変数四つを使っていかに高精度で分類できるか。\n",
        "従来の統計分類法では特徴変数同士をかけ合わせたりして、新しい特徴値を使って精度を高めていた。\n",
        "\n",
        "Q3.　irisデータとは3つのクラスの目的変数、4つの特徴変数をもつ150のデータ点からなるデータである。\n",
        "\n",
        "\n",
        "可視化した図\n",
        "\n",
        "入力層　４ノード\n",
        "\n",
        "中間層　1層\n",
        "\n",
        "出力層：　3ノード\n",
        "\n",
        "誤差関数: クロスエントロピー\n",
        "\n",
        "活性化関数:　中間層活性化関数：　ReLU\n",
        "\n",
        "出力層活性化関数：　Softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wOTAfqItOYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "499be6d0-6143-499c-c694-7cfb5c3350b3"
      },
      "source": [
        "リンク先URL：http://study-ai.com/jdla/\n",
        "\n",
        "バナー画像：http://ai999.careers/bnr_jdla.png"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b09512a4b2fc>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    リンク先URL：http://study-ai.com/jdla/\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    }
  ]
}