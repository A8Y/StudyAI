{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN_Day3_Report",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPPTLyzEU2QEfuY+BIwnJ1t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A8Y/StudyAI/blob/master/DNN_Day3_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u56ZYUDIvNc",
        "colab_type": "text"
      },
      "source": [
        "# DNN Day3 レポート\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSA7dR6hDw74",
        "colab_type": "text"
      },
      "source": [
        "# 再起型NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sJRtoc2I3Yw",
        "colab_type": "text"
      },
      "source": [
        "## Section1. RNN\n",
        "\n",
        "###RNN \n",
        "\n",
        "*  時系列データに対応可能なNN\n",
        "\n",
        "*  時系列データとは時間的・空間的・などで順列するデータで前の値と次の値には何らかの統計的依存関係がある\n",
        "\n",
        "###BPTT  (**B**ack **P**ropagation **T**hrough **T**ime)\n",
        "\n",
        "*  RNNの逆伝播勾配法\n",
        "\n",
        "*  誤差関数を連鎖率をつかって各パラメータで偏微分しそれぞれを更新\n",
        "\n",
        "####BPTTの課題\n",
        "\n",
        "*　勾配消失問題：学習が進むにつれて勾配がゼロに近づきパラメータ更新がほとんどなくなり学習ができない\n",
        "\n",
        "*　勾配爆発：　層を逆伝播するにつれて勾配が指数関数的に大きくなる。RNNが深い問題で起きやすい。\n",
        "\n",
        "*　クリッピングで閾値を越えたときに閾値をかけて爆発を防ぐ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw07wCNjJwe-",
        "colab_type": "text"
      },
      "source": [
        "## Section2. LSTM\n",
        "\n",
        "###CEC\n",
        "\n",
        "*  勾配爆発の解決法\n",
        "\n",
        "*  勾配が常に１になるために挿入された自己ループ\n",
        "\n",
        "*  時間依存度に関係なく重みが一律であるのでNNの学習特性が失われてしまう\n",
        "\n",
        "*  入力重み衝突\n",
        "\n",
        "*  出力重み衝突\n",
        "\n",
        "###入力ゲートと出力ゲート\n",
        "\n",
        "*  それぞれのゲートへの入力値の重みを重み行列W,Uで可変可能とするゲート\n",
        "\n",
        "###忘却ゲート\n",
        "\n",
        "*  いらなくなった過去の情報をそのタイミングで忘却する\n",
        "\n",
        "###のぞき穴結合\n",
        "\n",
        "*  CEC自身の値に重み行列を介して伝播可能にした構造\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91XDY3coJwji",
        "colab_type": "text"
      },
      "source": [
        "## Section3.　GRU (gated Recurrent Unit)\n",
        "\n",
        "*　LSTMではパラメータが多く計算負荷が高くなる問題を解決するための方法\n",
        "\n",
        "*　精度はLSTMか同等かそれ以上\n",
        "\n",
        "*　計算負荷が低い\n",
        "\n",
        "*　"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTJu0_mYJwnb",
        "colab_type": "text"
      },
      "source": [
        "## Section4．双方向RNN\n",
        "\n",
        "過去の情報だけではなく未来の情報を加味することで精度を向上させるためのモデル\n",
        "\n",
        "実用例：機械翻訳や文章推敲"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN3EaS2KDsAI",
        "colab_type": "text"
      },
      "source": [
        "# RNNでの自然言語処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSMT0ZEvJwqo",
        "colab_type": "text"
      },
      "source": [
        "## Section５．seq2seq \n",
        "\n",
        "Encoder-Decoderモデルの一種\n",
        "\n",
        "####Encoder RNN\n",
        "\n",
        "1. Taking:　文章を単語等のトークン毎に分割し、トークン毎のIDに分割する\n",
        "2. Embedding:　IDからそのトークンを表す分散表現ベクトルに変換\n",
        "3. Encoder RNN:　ベクトルを順番にRNNに入力していく\n",
        "\n",
        "\n",
        "####Decoder RNN\n",
        "\n",
        "システムがアウトプットデータを単語等のトークン毎に生成する構造\n",
        "\n",
        "2. Sampling: 生成確立に基づいてトーケンをランダムに選出\n",
        "\n",
        "3. Embedding:　２で選ばれたtokenをembeddingしてDecoder RNNへ次の入力とする\n",
        "\n",
        "4. Detokenize:　１－３を繰り返し、２で得られたtokenを文字列に直す\n",
        "\n",
        "Decoder RNNの処理手順\n",
        "\n",
        "\n",
        "\n",
        "####HRED\n",
        "\n",
        "* Seq2Seqは問いに対して文脈がなく、ただ単純な応答が行われる（一問一答）\n",
        "\n",
        "* 前の単をの流れに即して応答されるので、より人間らしい文章が生成される\n",
        "\n",
        "* Seq2Seq＋Context RNN\n",
        "\n",
        "* HREDは短く情報量に乏しい同じ答えをしがち\n",
        "\n",
        "####VHRED\n",
        "\n",
        "*　HREDにVAEの潜在変数の概念を追加することで、毎回同じだった答えにバリエーションを持たせることができる\n",
        "\n",
        "\n",
        "####VAE\n",
        "\n",
        "*　オートエンコーダ―\n",
        "\n",
        "        *  教師無し学習の一つ\n",
        "\n",
        "        *  入力データから潜在変数zに変換するNNをEncoder\n",
        "\n",
        "        *  潜在変数zをインプットとして、元画像を復元するNNをDecoder\n",
        "\n",
        "        *  次元削減効果がある\n",
        "\n",
        "\n",
        "*　VAE\n",
        "\n",
        "        *  潜在変数ｚに標準正規確率分布を当てはめたもの　z～N(1,０）\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mowA2KI4Ho7G",
        "colab_type": "text"
      },
      "source": [
        "## Section6.　Word2Vec \n",
        "\n",
        "RNNでは単語のような可変長の文字列をNNに与えることができないが、Word2Vecでは固定長形式で単語を表すことができる\n",
        "\n",
        "学習データからボキャブラリを作成\n",
        "\n",
        "OneHotベクターでそれぞれの単語を表現\n",
        "\n",
        "大規模データの分散表現の学習が現実的な計算速度とメモリ量で実現可能になった\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bh0YgfPWiHW",
        "colab_type": "text"
      },
      "source": [
        "## Section7. Attention Mechanism\n",
        "\n",
        "Seq2Seqでは長い文章への対応が難しいが、文章が長くなるほどそのシーケンスの内部表現の次元も大きくなっていっても対応できる仕組み\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azNYjRHlI5wA",
        "colab_type": "text"
      },
      "source": [
        "## 確認テスト\n",
        "\n",
        "【ｐ11】\n",
        "$O = \\frac{H+2P-F}{S}+1 = \\frac{5+2*1-3}{2}+1 = 3$\n",
        "\n",
        "出力画像：３ｘ３\n",
        "\n",
        "【ｐ23】隠れ層から次の中間層にわたされる値にかかる重み\n",
        "\n",
        "【ｐ36】$z=t^2 \\quad t=x+y \\quad \\rightarrow \\quad \\frac{dz}{dx} = \\frac{dz}{dt}\\frac{dt}{dx} = 2t(1) = 2(x+y)$\n",
        "\n",
        "【ｐ45】$$z_1 = sigmoid(s_0 W + x_1 W_{in} +b$$\n",
        "$$y_1 = sigmoid(  z_1  W_{out}+ c)$$\n",
        "\n",
        "【ｐ62】（2）\n",
        "\n",
        "【ｐ78】　忘却ゲート\n",
        "\n",
        "【ｐ88】　\n",
        "\n",
        "*　LSTMではパラメータが多く計算負荷が高くなる\n",
        "\n",
        "*　CECでは時間依存度に関係なく重みが一律であるのでNNの学習特性が失われてしまう\n",
        "\n",
        "【ｐ92】\n",
        "\n",
        "LSTMは全ての情報を保存する（パラメータ数が多い）ので計算負荷がおおきくなるが、GRUはいらなくなった情報（パラメータ）を必要でなくなったときに消去することができる\n",
        "\n",
        "\n",
        "【ｐ109】（２）\n",
        "\n",
        "【ｐ119】　HREDでは同じ質問には毎回同じ答えしか出せなかったが、VHREDではlatent変数の概念を追加することにより、答えにバリエーションを持たすことができる\n",
        "\n",
        "【ｐ128】標準正規確率分布\n",
        "\n",
        "【ｐ137】Seq2Seqでは長い文章への対応が難しいが、文章が長くなるほどそのシーケンスの内部表現の次元も大きくなっていっても対応できる仕組み\n",
        "\n"
      ]
    }
  ]
}